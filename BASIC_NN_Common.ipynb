{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D, Flatten, GlobalMaxPooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print(\"Train shape : \",train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill up the missing values\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "train_Y = np.array(train_df[\"target\"])\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "\n",
    "## split to train and val\n",
    "train_size = 50000\n",
    "\n",
    "my_indices = (np.random.permutation( len(train_X)))\n",
    "\n",
    "\n",
    "val_X = [train_X[i] for i in my_indices[train_size:] ]\n",
    "train_X = [train_X[i] for i in my_indices[0:train_size] ]\n",
    "\n",
    "val_Y = [train_Y[i] for i in my_indices[train_size:] ]\n",
    "train_Y = [train_Y[i] for i in my_indices[0:train_size] ]\n",
    "\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "\n",
    "np.save('saved_arrays/subsampled_train_X', train_X)\n",
    "np.save('saved_arrays/subsampled_train_Y', train_Y)\n",
    "np.save('saved_arrays/subsampled_val_X', val_X)\n",
    "np.save('saved_arrays/subsampled_val_Y', val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.load('saved_arrays/subsampled_train_X.npy' )\n",
    "train_Y = np.load('saved_arrays/subsampled_train_Y.npy' )\n",
    "val_X = np.load('saved_arrays/subsampled_val_X.npy' )\n",
    "val_Y = np.load('saved_arrays/subsampled_val_Y.npy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X_reduced =val_X[0:50000]\n",
    "val_Y_reduced =val_Y[0:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITHOUT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length = maxlen ))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)\n",
    "\n",
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_no_embeddings/dense_thresh_32.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('saved_models_no_embeddings/dense_thresh_32.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length = maxlen ))\n",
    "    \n",
    "    model.add(Conv1D(128, 2, activation='relu'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for thresh in np.arange(0.01, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_no_embeddings/CNN_thresh_29.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('saved_models_no_embeddings/CNN_thresh_29.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, input_length = maxlen ))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in np.arange(0.01, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_no_embeddings/LSTM_thresh_25.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('saved_models_no_embeddings/LSTM_thresh_25.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding = 'utf-8'))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "np.save('saved_arrays/glove_embedding_matrix', embedding_matrix)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('saved_arrays/glove_embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, binary=True))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "np.save('saved_arrays/google_embedding_matrix', embedding_matrix)        \n",
    "embedding_matrix = np.load('saved_arrays/google_embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DENSE ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in np.arange(0.01, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_with_embeddings/Dense_thresh_23.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('saved_models_with_embeddings/Dense_thresh_23.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "    \n",
    "    model.add(Conv1D(128, 2, activation='relu'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in np.arange(0.01, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_with_embeddings/CNN_thresh_48.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('saved_models_with_embeddings/CNN_thresh_48.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for thresh in np.arange(0.01, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_with_embeddings/LSTM_thresh_25.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('saved_models_with_embeddings/LSTM_thresh_25.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['step_dim'] = self.step_dim # say self. _localization_net  if you store the argument in __init__\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DENSE NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Attention(maxlen))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in np.arange(0.01, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_with_attention_embed/Dense_thresh_26.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('saved_models_with_attention_embed/Dense_thresh_26.h5', custom_objects={'Attention': Attention})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "def model_compile():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Attention(maxlen))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = model_compile()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in np.arange(0.01, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_Y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('saved_models_with_attention_embed/LSTM_thresh_23.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('saved_models_with_attention_embed/LSTM_thresh_23.h5', custom_objects={'Attention': Attention})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model here     \n",
    "class F1Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valf1 = []\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            y_pred = (y_pred > 0.48).astype(int)\n",
    "            score = metrics.f1_score(self.y_val, y_pred)\n",
    "            print(score)\n",
    "            self.valf1.append(score)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "model.add(Conv1D(128, 2, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = F1Evaluation(validation_data=(val_X_reduced, val_Y_reduced), interval=10)\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1, callbacks=[history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_valf1 = history.valf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_valf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='CNN_arch.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session \n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model here\n",
    "from keras.callbacks import Callback\n",
    "class F1Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valf1 = []\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            y_pred = (y_pred > 0.23).astype(int)\n",
    "            score = metrics.f1_score(self.y_val, y_pred)\n",
    "            print(score)\n",
    "            self.valf1.append(score)\n",
    "\n",
    "            \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = F1Evaluation(validation_data=(val_X_reduced, val_Y_reduced), interval=10)\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, verbose = 1, callbacks=[history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense_val_f1 = history.valf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='Dense_arch.png', show_shapes=True, show_layer_names=True)\n",
    "from keras.backend import clear_session \n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model here\n",
    "        \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = LossHistory()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_loss = history.losses\n",
    "LSTM_val_f1 = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='LSTM_arch.png', show_shapes=True, show_layer_names=True)\n",
    "from keras.backend import clear_session \n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model here\n",
    "    \n",
    "        \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen ))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Attention(maxlen))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = LossHistory()\n",
    "model.fit(train_X, train_Y, batch_size=256, epochs=2, validation_data=(val_X_reduced, val_Y_reduced), verbose = 1, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_wa_loss = history.losses\n",
    "LSTM_wa_val_loss = history.val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='LSTM_wa_arch.png', show_shapes=True, show_layer_names=True)\n",
    "from keras.backend import clear_session \n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_wa_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "pl.rcParams['figure.figsize']  = 10, 7.5\n",
    "pl.rcParams['lines.linewidth'] = 1.5\n",
    "pl.rcParams['font.family']     = 'serif'\n",
    "pl.rcParams['font.weight']     = 'bold'\n",
    "pl.rcParams['font.size']       = 20\n",
    "pl.rcParams['font.sans-serif'] = 'serif'\n",
    "pl.rcParams['text.usetex']     = True\n",
    "pl.rcParams['axes.linewidth']  = 1.5\n",
    "pl.rcParams['axes.titlesize']  = 'medium'\n",
    "pl.rcParams['axes.labelsize']  = 'medium'\n",
    "\n",
    "pl.rcParams['xtick.major.size'] = 8\n",
    "pl.rcParams['xtick.minor.size'] = 4\n",
    "pl.rcParams['xtick.major.pad']  = 8\n",
    "pl.rcParams['xtick.minor.pad']  = 8\n",
    "pl.rcParams['xtick.color']      = 'k'\n",
    "pl.rcParams['xtick.labelsize']  = 'medium'\n",
    "pl.rcParams['xtick.direction']  = 'in'\n",
    "\n",
    "pl.rcParams['ytick.major.size'] = 8\n",
    "pl.rcParams['ytick.minor.size'] = 4\n",
    "pl.rcParams['ytick.major.pad']  = 8\n",
    "pl.rcParams['ytick.minor.pad']  = 8\n",
    "pl.rcParams['ytick.color']      = 'k'\n",
    "pl.rcParams['ytick.labelsize']  = 'medium'\n",
    "pl.rcParams['ytick.direction']  = 'in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(len(LSTM_wa_loss)))\n",
    "\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "xnew = np.linspace(x.min(),x.max(),100) #300 represents number of points to make between T.min and T.max\n",
    "\n",
    "power_smooth_LSTM_wa_loss = spline(x,LSTM_wa_loss,xnew)\n",
    "power_smooth_LSTM_loss = spline(x,LSTM_loss,xnew)\n",
    "power_smooth_CNN_loss = spline(x,CNN_loss,xnew)\n",
    "power_smooth_Dense_loss = spline(x,Dense_loss,xnew)\n",
    "\n",
    "\n",
    "pl.plot(xnew,power_smooth_LSTM_wa_loss, '-', label = 'LSTM with attention layer')\n",
    "pl.plot(xnew,power_smooth_LSTM_loss, '-', label = 'LSTM')\n",
    "pl.plot(xnew,power_smooth_CNN_loss, '-', label = 'CNN')\n",
    "pl.plot(xnew,power_smooth_Dense_loss, '-', label = 'Feed Foward Network')\n",
    "pl.legend()\n",
    "pl.xlabel('Batches')\n",
    "pl.ylabel('Binary Cross Entropy Loss')\n",
    "pl.title('Training Loss Progress with incoming batches')\n",
    "pl.savefig('TrainingLoss.png')\n",
    "pl.show()\n",
    "pl.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
